@inproceedings{10.1145/3512527.3531439,
author = {Gurrin, Cathal and Zhou, Liting and Healy, Graham and \TH{}\'{o}r J\'{o}nsson, Bj\"{o}rn and Dang-Nguyen, Duc-Tien and Loko\'{c}, Jakub and Tran, Minh-Triet and H\"{u}rst, Wolfgang and Rossetto, Luca and Sch\"{o}ffmann, Klaus},
title = {Introduction to the Fifth Annual Lifelog Search Challenge, LSC'22},
year = {2022},
isbn = {9781450392389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512527.3531439},
doi = {10.1145/3512527.3531439},
abstract = {For the fifth time since 2018, the Lifelog Search Challenge (LSC) facilitated a benchmarking exercise to compare interactive search systems designed for multimodal lifelogs. LSC'22 attracted nine participating research groups who developed interactive lifelog retrieval systems enabling fast and effective access to lifelogs. The systems competed in front of a hybrid audience at the LSC workshop at ACM ICMR'22. This paper presents an introduction to the LSC workshop, the new (larger) dataset used in the competition, and introduces the participating lifelog search systems.},
booktitle = {Proceedings of the 2022 International Conference on Multimedia Retrieval},
pages = {685–687},
numpages = {3},
keywords = {interactive retrieval systems, benchmarking, lifelog},
location = {Newark, NJ, USA},
series = {ICMR '22}
},
@misc{https://doi.org/10.48550/arxiv.2103.00020,
  doi = {10.48550/ARXIV.2103.00020},
  
  url = {https://arxiv.org/abs/2103.00020},
  
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
},
@inproceedings{10.1145/3512729.3533005,
author = {Leibetseder, Andreas and Stefanics, Daniela and Schoeffmann, Klaus},
title = {LifeXplore at the Lifelog Search Challenge 2022},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533005},
doi = {10.1145/3512729.3533005},
abstract = {Lifelogging creates substantial data archives that are challenging to manage and search. The annual Lifelog Search Challenge (LSC) aims at improving this situation by encouraging international teams to create interactive retrieval systems for searching large lifelog databases. The LSC challenge is held as a live event co-located at the ACM International Conference on Multimedia Retrieval (ICMR), where participating teams compete against each other by solving time-based retrieval tasks. In this paper, we present an improved version of lifeXplore -- our system already participating since LSC2018. For LSC2022, we focus on improving the result presentation as well as the system's interface.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {48–52},
numpages = {5},
keywords = {image search, interactive image retrieval, lifelogging, evaluation campaign},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@article{DBLP:journals/corr/abs-2004-10934,
  author    = {Alexey Bochkovskiy and
               Chien{-}Yao Wang and
               Hong{-}Yuan Mark Liao},
  title     = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  journal   = {CoRR},
  volume    = {abs/2004.10934},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.10934},
  eprinttype = {arXiv},
  eprint    = {2004.10934},
  timestamp = {Tue, 28 Apr 2020 16:10:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-10934.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},
@inproceedings{10.1145/3463948.3469064,
author = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Thanh Binh, Nguyen and Lee, Hyowon and Gurrin, Cathal},
title = {Mysc\'{e}al 2.0: A Revised Experimental Interactive Lifelog Retrieval System for LSC'21},
year = {2021},
isbn = {9781450385336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463948.3469064},
doi = {10.1145/3463948.3469064},
abstract = {Building an interactive retrieval system for lifelogging contains many challenges due to massive multi-modal personal data besides the requirement of accuracy and rapid response for such a tool. The Lifelog Search Challenge (LSC) is the international lifelog retrieval competition that inspires researchers to develop their systems to cope with the challenges and evaluates the effectiveness of their solutions. In this paper, we upgrade our previous Mysc\'{e}al and present Mysc\'{e}al 2.0 system for the LSC'21 with the improved features inspired by the novice users experiments. The experiments show that a novice user achieved more than half of the expert score on average. To mitigate the gap of them, some potential enhancements were identified and integrated to the enhanced version.},
booktitle = {Proceedings of the 4th Annual on Lifelog Search Challenge},
pages = {11–16},
numpages = {6},
keywords = {elasticsearch, interactive retrieval system, lifelog},
location = {Taipei, Taiwan},
series = {LSC '21}
},
@inproceedings{7780677,
author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Rethinking the Inception Architecture for Computer Vision}, 
year={2016},
volume={},
number={},
pages={2818-2826},
doi={10.1109/CVPR.2016.308}
},
@inproceedings{10.1145/3512729.3533003,
author = {Heller, Silvan and Rossetto, Luca and Sauter, Loris and Schuldt, Heiko},
title = {Vitrivr at the Lifelog Search Challenge 2022},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533003},
doi = {10.1145/3512729.3533003},
abstract = {In this paper, we present the iteration of the multimedia retrieval system vitrivr participating at LSC 2022. vitrivr is a general-purpose retrieval system which has previously participated at LSC. We describe the system architecture and functionality, and show initial results based on the test and validation topics.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {27–31},
numpages = {5},
keywords = {multimedia retrieval, lifelog search challenge, lifelogging, content-based retrieval},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533008,
author = {Spiess, Florian and Schuldt, Heiko},
title = {Multimodal Interactive Lifelog Retrieval with Vitrivr-VR},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533008},
doi = {10.1145/3512729.3533008},
abstract = {The multimodal nature of lifelog data poses unique challenges for analysis, indexing and interactive retrieval. To address these challenges, the Lifelog Search Challenge (LSC) is an annual evaluation campaign allowing interactive retrieval systems to explore new ideas and measure their performance against each other.This paper describes the virtual reality (VR) multimedia retrieval system vitrivr-VR, with a focus on aspects relevant to the LSC'22, especially the user interaction in VR, the formulation of typical LSC queries, and different options to explore the retrieval results in VR.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {38–42},
numpages = {5},
keywords = {virtual reality, interactive lifelog retrieval, lifelog search challenge},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533006,
author = {Alam, Naushad and Graham, Yvette and Gurrin, Cathal},
title = {Memento 2.0: An Improved Lifelog Search Engine for LSC'22},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533006},
doi = {10.1145/3512729.3533006},
abstract = {In this paper, we present Memento 2.0, an improved version of our system which first participated in the Lifelog Search Challenge 2021. Memento 2.0 employs image-text embeddings derived from two CLIP models (ViT-L/14 and ResNet-50x64) and adopts a weighted ensemble approach to derive a combined final ranking. Our approach significantly improves the performance over the baseline LSC'21 system. We additionally make important updates to the system's user interface after analysing the shortcomings to make it more efficient and better suited to the needs of the Lifelog Search Challenge.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {2–7},
numpages = {6},
keywords = {search interfaces, retrieval models and ranking, information systems},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533009,
author = {Alateeq, Ahmed and Roantree, Mark and Gurrin, Cathal},
title = {Voxento 3.0: A Prototype Voice-Controlled Interactive Search Engine for Lifelog},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533009},
doi = {10.1145/3512729.3533009},
abstract = {Voxento is an interactive voice-based retrieval system for lifelogs which has been redeveloped and optimised to participate in the fifth Lifelog Search Challenge LSC'22, at ACM ICMR'22. Based on the previous experience in the LSC competition and ranked in the top 4 in the last LSC'21 competition among 17 participants, we present a revised version of Voxento to address the critical points to improve the efficiency of retrieval tasks in lifelog datasets. Basically, Voxento provides a spoken interface to the lifelog data, which facilitates an expert and novice user to interact with a personal lifelog using a range of vocal commands and interactions. Briefly, we made some important improvements to support both the retrieval of content and system interaction. This latest version has been enhanced with the addition of a text-based search feature, new filters based on new metadata provided in lifelog data, rich visual information and features and enhanced speech query. Also, the data preparation tasks comprised a new function to reduce the number of non-relevant images and the latest CLIP model version used to derive features from images. The long term development of Voxento includes a lifelog retrieval that supports speech and conversation interaction with less physical actions required by users such as using a mouse. The system presented here uses a desktop computer in order to participate in the LSC'22 competition with the option to use voice interaction or standard text-based retrieval.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {43–47},
numpages = {5},
keywords = {speech recognition, lifelog, interactive retrieval, speech synthesis, voice interaction},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533014,
author = {Nguyen, Thao-Nhu and Le, Tu-Khiem and Ninh, Van-Tu and Tran, Minh-Triet and Nguyen, Thanh Binh and Healy, Graham and Smyth, Sin\'{e}ad and Caputo, Annalina and Gurrin, Cathal},
title = {LifeSeeker 4.0: An Interactive Lifelog Search Engine for LSC'22},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533014},
doi = {10.1145/3512729.3533014},
abstract = {In this paper, we introduce LifeSeeker 4.0 - an interactive lifelog retrieval system developed for the fifth annual Lifelog Search Challenge (LSC'22). In LifeSeeker 4.0, we focus on enhancing our previous system to allow users who have little to no knowledge of underlying system functioning and lifelog data to use it with ease by employing a Contrastive Language-Image Pre-training (CLIP) model. Furthermore, we have exploited the music metadata to facilitate searches that may incorporate emotion. Event clustering is also improved in this version to increase user experience by reducing the occurrence of repeated images, and hence decreasing the search time.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {14–19},
numpages = {6},
keywords = {interactive retrieval, lifelog, information system},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533013,
author = {Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Nguyen, E-Ro and Le, Thanh-Cong and Tran, Mai-Khiem and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal and Tran, Minh-Triet},
title = {Flexible Interactive Retrieval SysTem 3.0 for Visual Lifelog Exploration at LSC 2022},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533013},
doi = {10.1145/3512729.3533013},
abstract = {Building a retrieval system with lifelogging data is more complicated than with ordinary data due to the redundancies, blurriness, massive amount of data, various sources of information accompanying lifelogging data, and especially the ad-hoc nature of queries. The Lifelog Search Challenge (LSC) is a benchmarking challenge that encourages researchers and developers to push the boundaries in lifelog retrieval. For LSC'22, we develop FIRST 3.0, a novel and flexible system that leverages expressive cross-domain embeddings to enhance the searching process. Our system aims to adaptively capture the semantics of an image at different levels of detail. We also propose to augment our system with an external search engine to help our system with initial visual examples for unfamiliar concepts. Finally, we organize image data in hierarchical clusters based on their visual similarity and location to assist users in data exploration. Experiments show that our system is both fast and effective in handling various retrieval scenarios.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {20–26},
numpages = {7},
keywords = {query expansion, semantic embedding, interactive retrieval systems, lifelog},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533012,
author = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Nguyen, Binh and Lee, Hyowon and Zhou, Liting and Gurrin, Cathal},
title = {E-Mysc\'{e}al: Embedding-Based Interactive Lifelog Retrieval System for LSC'22},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533012},
doi = {10.1145/3512729.3533012},
abstract = {Developing interactive lifelog retrieval systems is a growing research area. There are many international competitions for lifelog retrieval that encourage researchers to build effective systems that can address the multimodal retrieval challenge of lifelogs. The Lifelog Search Challenge (LSC) was first organised in 2018 and is currently the only interactive benchmarking evaluation for lifelog retrieval systems. Participating systems should have an accurate search engine and a user-friendly interface that can help users to retrieve relevant content. In this paper, we upgrade our previous MySc\'{e}al, which was the top performing system in LSC'20 and LSC'21, and present E-MySc\'{e}al for LSC'22, which includes a completely different search engine. Instead of using visual concepts for retrieval such as MySc\'{e}al, the new E-MySc\'{e}al employs an embedding technique that facilitates novice users who are not familiar with the concepts. Our experiments show that the new search engine can find relevant images in the first place in the ranked list, four a quarter of the LSC'21 queries (26\%) by using just the first hint from the textual information need. Regarding the user interface, we still keep the simple non-faceted design as in the previous version but improve the event view browsing in order to better support novice users.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {32–37},
numpages = {6},
keywords = {lifelog, human factors, interactive retrieval system},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@inproceedings{10.1145/3512729.3533011,
author = {Ribiero, Ricardo and Trifan, Alina and Neves, Antonio J. R.},
title = {MEMORIA: A Memory Enhancement and MOment RetrIeval Application for LSC 2022},
year = {2022},
isbn = {9781450392396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512729.3533011},
doi = {10.1145/3512729.3533011},
abstract = {Research on retrieving data and analyzing lifelogs revealed to be a very complex task, and the interdisciplinary challenges to be tackled have boosted increasing attention from the scientific community in information retrieval and lifelogging. The Lifelog Search Challenge is an international competition for lifelog retrieval in which researchers propose their approaches and compete to solve lifelog retrieval challenges and evaluate the effectiveness of their systems. In this paper, we present the MEMORIA computational tool to participate for the first time in the Lifelog Search Challenge 2022. The information retrieval is based on the search of keywords and time periods and several computer vision methods are used to process visual lifelogs, from pre-processing algorithms to feature extraction methods, in order to enrich the annotation of the lifelogs. Preliminary experimental results of the user interaction with our retrieval module are presented, confirming the effectiveness of the proposed approach and showing the most relevant functionalities of the system.},
booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
pages = {8–13},
numpages = {6},
keywords = {data retrieval, lifelog, object detection, machine learning, information systems, lifelogging, image processing, image annotation},
location = {Newark, NJ, USA},
series = {LSC '22}
},
@misc{https://doi.org/10.48550/arxiv.2207.02696,
  doi = {10.48550/ARXIV.2207.02696},
  url = {https://arxiv.org/abs/2207.02696},
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
},
@misc{https://doi.org/10.48550/arxiv.1405.0312,
  doi = {10.48550/ARXIV.1405.0312},
  url = {https://arxiv.org/abs/1405.0312},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Microsoft COCO: Common Objects in Context},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
},
@misc{https://doi.org/10.48550/arxiv.2111.09734,
  doi = {10.48550/ARXIV.2111.09734},
  url = {https://arxiv.org/abs/2111.09734},
  author = {Mokady, Ron and Hertz, Amir and Bermano, Amit H.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ClipCap: CLIP Prefix for Image Captioning},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
},
@inproceedings{10.1007/978-3-030-98358-1_18,
author = {Tran, Ly-Duyen and Ho, Thanh Cong and Pham, Lan Anh and Nguyen, Binh and Gurrin, Cathal and Zhou, Liting},
title = {LLQA - Lifelog Question Answering Dataset},
year = {2022},
isbn = {978-3-030-98357-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-98358-1_18},
doi = {10.1007/978-3-030-98358-1_18},
abstract = {Recollecting details from lifelog data involves a higher level of granularity and reasoning than a conventional lifelog retrieval task. Investigating the task of Question Answering (QA) in lifelog data could help in human memory recollection, as well as improve traditional lifelog retrieval systems. However, there has not yet been a standardised benchmark dataset for the lifelog-based QA. In order to provide a first dataset and baseline benchmark for QA on lifelog data, we present a novel dataset, LLQA, which is an augmented 85-day lifelog collection and includes over 15,000 multiple-choice questions. We also provide different baselines for the evaluation of future works. The results showed that lifelog QA is a challenging task that requires more exploration. The dataset is publicly available at .},
booktitle = {MultiMedia Modeling: 28th International Conference, MMM 2022, Phu Quoc, Vietnam, June 6–10, 2022, Proceedings, Part I},
pages = {217–228},
numpages = {12},
keywords = {Question answering, Lifelogging},
location = {Phu Quoc, Vietnam}
},
@misc{https://doi.org/10.48550/arxiv.1904.01941,
  doi = {10.48550/ARXIV.1904.01941},
  url = {https://arxiv.org/abs/1904.01941},
  author = {Baek, Youngmin and Lee, Bado and Han, Dongyoon and Yun, Sangdoo and Lee, Hwalsuk},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Character Region Awareness for Text Detection},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
},
@article{XIE2020107205,
title = {Scene recognition: A comprehensive survey},
journal = {Pattern Recognition},
volume = {102},
pages = {107205},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107205},
url = {https://www.sciencedirect.com/science/article/pii/S003132032030011X},
author = {Lin Xie and Feifei Lee and Li Liu and Koji Kotani and Qiu Chen},
keywords = {Scene recognition, Patch feature encoding, Spatial layout pattern learning, Discriminative region detection, Convolutional neural networks, Deep learning},
abstract = {With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted.}
},
@misc{https://doi.org/10.48550/arxiv.1907.07570,
  doi = {10.48550/ARXIV.1907.07570},
  url = {https://arxiv.org/abs/1907.07570},
  author = {Seong, Hongje and Hyun, Junhyuk and Kim, Euntai},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FOSNet: An End-to-End Trainable Deep Neural Network for Scene Recognition},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
},

@ARTICLE{7968387,
author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Places: A 10 Million Image Database for Scene Recognition}, 
year={2018},
volume={40},
number={6},
pages={1452-1464},
doi={10.1109/TPAMI.2017.2723009}},
@inproceedings{10.1145/3460426.3463607,
author = {Yen, An-Zi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Ten Questions in Lifelog Mining and Information Recall},
year = {2021},
isbn = {9781450384636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460426.3463607},
doi = {10.1145/3460426.3463607},
abstract = {With the advance of science and technology, people are used to recording their daily life events via writing blogs, uploading social media posts, taking photos, or filming videos. Such rich repository personal information is useful for supporting human living assistance, such as information recall service. The main challenges are how to store and manage personal knowledge from various sources, and how to provide support for people who may have difficulty recalling past experiences. In this position paper, we propose a research agenda on personal knowledge mining from various sources of lifelogs, personal knowledge base construction, and information recall for assisting people to recall their experiences. Ten research questions are formulated.},
booktitle = {Proceedings of the 2021 International Conference on Multimedia Retrieval},
pages = {511–518},
numpages = {8},
keywords = {information recall, life event extraction, personal knowledge base construction},
location = {Taipei, Taiwan},
series = {ICMR '21}
},
@article{L_pez_Cifuentes_2020,
	doi = {10.1016/j.patcog.2020.107256},
	url = {https://doi.org/10.1016/j.patcog.2020.107256},
	year = 2020,
	month = {jun},
	publisher = {Elsevier {BV}},
	volume = {102},
	pages = {107256},
	author = {López-Cifuentes, Alejandro and Escudero-Viñolo, Marcos and Bescós, Jesús and Garcia-Martin, Alvaro},
	title = {Semantic-aware scene recognition},
	journal = {Pattern Recognition}
},
@ARTICLE{8290727,
author={Kim, Seongjung and Yeom, Seongkyu and Kwon, Oh-Jin and Shin, Dongil and Shin, Dongkyoo},
journal={IEEE Access}, 
title={Ubiquitous Healthcare System for Analysis of Chronic Patients’ Biological and Lifelog Data}, 
year={2018},
volume={6},
number={},
pages={8909-8915},
doi={10.1109/ACCESS.2018.2805304}
},
@article{104630091011,
author = {Kumagai, Narimasa and Tajika, Aran and Hasegawa, Akio and Kawanishi, Nao and Horikoshi, Masaru and Shimodera, Shinji and Kurata, Ken’ichi and Chino, Bun and Furukawa, Toshi},
year = {2019},
month = {12},
pages = {391},
title = {Predicting recurrence of depression using lifelog data: an explanatory feasibility study with a panel VAR approach},
volume = {19},
journal = {BMC Psychiatry},
doi = {10.1186/s12888-019-2382-2}
},
@article{articleArrhythmia,
author = {Shin, Siho and Jung, Jaehyo and Kang, Mingu and Kim, Youn},
year = {2021},
month = {01},
pages = {1-3},
title = {Arrhythmia Detection Algorithm using GoogLeNet and Generative Adversarial Network with Lifelog Signals},
volume = {15},
journal = {International Journal of Biology and Biomedical Engineering},
doi = {10.46300/91011.2021.15.1}
},
@article{101136bmjopen,
author = {Sugawara, Junichi and Ochi, Daisuke and Yamashita, Riu and Yamauchi, Takafumi and Saigusa, Daisuke and Wagata, Maiko and Obara, Taku and Ishikuro, Mami and Tsunemoto, Yoshiki and Harada, Yuki and Shibata, Tomoko and Mimori, Takahiro and Kawashima, Junko and Katsuoka, Fumiki and Igarashi-Takai, Takako and Ogishima, Soichi and Metoki, Hirohito and Hashizume, Hiroaki and Fuse, Nobuo and Nagasaki, Masao},
year = {2019},
month = {02},
pages = {bmjopen-2018},
title = {Maternity Log study: A longitudinal lifelog monitoring and multiomics analysis for the early prediction of complicated pregnancy},
volume = {9},
journal = {BMJ Open},
doi = {10.1136/bmjopen-2018-025939}
}, @INPROCEEDINGS{9671403,
author={La, Tuan-Vinh and Dao, Minh-Son and Tejima, Kazuki and Kiran, Rage Uday and Zettsu, Koji},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
title={Improving the Awareness of Sustainable Smart Cities by Analyzing Lifelog Images and IoT Air Pollution Data}, 
year={2021},
volume={},
number={},
pages={3589-3594},
doi={10.1109/BigData52589.2021.9671403}
},
@inproceedings{10.1145/3279810.3279850,
author = {Dudzik, Bernd and Broekens, Joost and Neerincx, Mark and Olenick, Jeffrey and Chang, Chu-Hsiang and Kozlowski, Steve W. J. and Hung, Hayley},
title = {Discovering Digital Representations for Remembered Episodes from Lifelog Data},
year = {2018},
isbn = {9781450360722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279810.3279850},
doi = {10.1145/3279810.3279850},
abstract = {Combining self-reports in which individuals reflect on their thoughts and feelings (Experience Samples) with sensor data collected via ubiquitous monitoring can provide researchers and applications with detailed insights about human behavior and psychology. However, meaningfully associating these two sources of data with each other is difficult: while it is natural for human beings to reflect on their experience in terms of remembered episodes, it is an open challenge to retrace this subjective organization in sensor data referencing objective time.Lifelogging is a specific approach to the ubiquitous monitoring of individuals that can contribute to overcoming this recollection gap. It strives to create a comprehensive timeline of semantic annotations that reflect the impressions of the monitored person from his or her own subjective point-of-view.In this paper, we describe a novel approach for processing such lifelogs to situate remembered experiences in an objective timeline. It involves the computational modeling of individuals' memory processes to estimate segments within a lifelog acting as plausible digital representations for their recollections. We report about an empirical investigation in which we use our approach to discover plausible representations for remembered social interactions between participants in a longitudinal study. In particular, we describe an exploration of the behavior displayed by our model for memory processes in this setting. Finally, we explore the representations discovered for this study and discuss insights that might be gained from them.},
booktitle = {Proceedings of the Workshop on Modeling Cognitive Processes from Multimodal Data},
articleno = {13},
numpages = {9},
keywords = {wearables, recollection, cognitive modeling, lifelog, experience sampling, episodic memory, ubiquitous computing},
location = {Boulder, Colorado},
series = {MCPMD '18}
},
@article{li_know_2021,
	title = {Know {Yourself}: {Physical} and {Psychological} {Self}-{Awareness} {With} {Lifelog}},
	volume = {3},
	issn = {2673-253X},
	shorttitle = {Know {Yourself}},
	url = {https://www.frontiersin.org/articles/10.3389/fdgth.2021.676824/full},
	doi = {10.3389/fdgth.2021.676824},
	abstract = {Self-awareness is an essential concept in physiology and psychology. Accurate overall self-awareness benefits the development and well being of an individual. The previous research studies on self-awareness mainly collect and analyze data in the laboratory environment through questionnaires, user study, or field research study. However, these methods are usually not real-time and unavailable for daily life applications. Therefore, we propose a new direction of utilizing lifelog for self-awareness. Lifelog records about daily activities are used for analysis, prediction, and intervention on individual physical and psychological status, which can be automatically processed in real-time. With the help of lifelog, ordinary people are able to understand their condition more precisely, get effective personal advice about health, and even discover physical and mental abnormalities at an early stage. As the first step on using lifelog for self-awareness, we learn from the traditional machine learning problems, and summarize a schema on data collection, feature extraction, label tagging, and model learning in the lifelog scenario. The schema provides a flexible and privacy-protected method for lifelog applications. Following the schema, four topics were conducted: sleep quality prediction, personality detection, mood detection and prediction, and depression detection. Experiments on real datasets show encouraging results on these topics, revealing the significant relation between daily activity records and physical and psychological self-awareness. In the end, we discuss the experiment results and limitations in detail and propose an application,
              Lifelog Recorder
              , for multi-dimensional self-awareness lifelog data collection.},
	urldate = {2022-12-31},
	journal = {Frontiers in Digital Health},
	author = {Li, Jiayu and Ma, Weizhi and Zhang, Min and Wang, Pengyu and Liu, Yiqun and Ma, Shaoping},
	month = aug,
	year = {2021},
	pages = {676824},
},
@inproceedings{10.1145/3372278.3391934,
author = {Tran, Van-Luon and Mai-Nguyen, Anh-Vu and Phan, Trong-Dat and Vo, Anh-Khoa and Dao, Minh-Son and Zettsu, Koji},
title = {An Interactive Multimodal Retrieval System for Memory Assistant and Life Organized Support},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3391934},
doi = {10.1145/3372278.3391934},
abstract = {Lifelogging is known as the new trend of writing diary digitally where both the surrounding environment and personal physiological data and cognition are collected at the same time under the first perspective. Exploring and exploiting these lifelog (i.e., data created by lifelogging) can provide useful insights for human beings, including healthcare, work, entertainment, and family, to name a few. Unfortunately, having a valuable tool working on lifelog to discover these insights is still a tough challenge. To meet this requirement, we introduce an interactive multimodal retrieval system that aims to provide people with two functions, memory assistant and life organized support, with a friendly and easy-to-use web UI. The output of the former function is a video with footages expressing all instances of events people want to recall. The latter function generates a statistical report of each event so that people can have more information to balance their lifestyle. The system relies on two major algorithms that try to match keywords/phrases to images and to run a cluster-based query using a watershed-based approach.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {416–420},
numpages = {5},
keywords = {lifelogs, semantic, feature extracting, clustering, content and context, retrieval, image},
location = {Dublin, Ireland},
series = {ICMR '20}
},
@inproceedings{10.1145/3379172.3391717,
author = {Rossetto, Luca and Baumgartner, Matthias and Ashena, Narges and Ruosch, Florian and Pernischov\'{a}, Romana and Bernstein, Abraham},
title = {LifeGraph: A Knowledge Graph for Lifelogs},
year = {2020},
isbn = {9781450371360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379172.3391717},
doi = {10.1145/3379172.3391717},
abstract = {The data produced by efforts such as life logging is commonly multi modal and can have manifold interrelations with itself as well as external information. Representing this data in such a way that these rich relations as well as all the different sources can be leveraged is a non-trivial undertaking. In this paper, we present the first iteration of LifeGraph, a Knowledge Graph for lifelogging data. LifeGraph aims at not only capturing all aspects of the data contained in a lifelog but also linking them to external, static knowledge bases in order to put the log as a whole as well as its individual entries into a broader context. In the Lifelog Search Challenge 2020, we show a first proof-of-concept implementation of LifeGraph as well as a retrieval system prototype which utilizes it to search the log for specific events.},
booktitle = {Proceedings of the Third Annual Workshop on Lifelog Search Challenge},
pages = {13–17},
numpages = {5},
keywords = {graph-based retrieval, knowledge graphs, multi-modal retrieval, lifelog search challenge, lifelogging},
location = {Dublin, Ireland},
series = {LSC '20}
},
@inproceedings{10.1145/3391203.3391222,
author = {Morimoto, Mayo and Fujitsuka, Masashi and Mikami, Sawako and Motohashi, Yosuke},
title = {Video Lifelog Retrieval System for Ambiguous Search Queries},
year = {2020},
isbn = {9781450387682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391203.3391222},
doi = {10.1145/3391203.3391222},
abstract = {The fifth-generation mobile network (5G) will soon become popular, and data utilization will increase. Although there is a large volume of video data, making it difficult to manage and use such data in our daily lives, there is huge potential with such data. We now often take photos using smartphones, but in future recording our lives by wearing video cameras would become popular. Since such data have a large amount of information, a video lifelog retrieval system is necessary. Although many studies on video retrieval have been conducted on some domains, e.g. TV programs, movies and so on, there have been few conducted on lifelogs. One of the main differences between them is regarding user queries and the fact that our memories are often ambiguous. We propose a video lifelog retrieval system that takes advantage of ambiguous search queries. We discuss the effectiveness of our system from our evaluation involving our own lifelog data we collected. We also discuss the characteristics of lifelog data and problems of our system.},
booktitle = {Proceedings of the 2020 Symposium on Emerging Research from Asia and on Asian Contexts and Cultures},
pages = {65–68},
numpages = {4},
keywords = {First-Person Point of View, Video Retrieval, Computer Vision, Video Lifelog, Natural Language Processing},
location = {Honolulu, HI, USA},
series = {AsianCHI '20}
},
@ARTICLE{9386110,
  author={Adnan, Myasar Mundher and Rahim, Mohd Shafry Mohd and Rehman, Amjad and Mehmood, Zahid and Saba, Tanzila and Naqvi, Rizwan Ali},
  journal={IEEE Access}, 
  title={Automatic Image Annotation Based on Deep Learning Models: A Systematic Review and Future Challenges}, 
  year={2021},
  volume={9},
  number={},
  pages={50253-50264},
  doi={10.1109/ACCESS.2021.3068897}
},
@article{ZAIDI2022103514,
title = {A survey of modern deep learning based object detection models},
journal = {Digital Signal Processing},
volume = {126},
pages = {103514},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103514},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422001312},
author = {Syed Sahil Abbas Zaidi and Mohammad Samar Ansari and Asra Aslam and Nadia Kanwal and Mamoona Asghar and Brian Lee},
keywords = {Object detection and recognition, Convolutional neural networks (CNN), Lightweight networks, Deep learning},
abstract = {Object Detection is the task of classification and localization of objects in an image or video. It has gained prominence in recent years due to its widespread applications. This article surveys recent developments in deep learning based object detectors. Concise overview of benchmark datasets and evaluation metrics used in detection is also provided along with some of the prominent backbone architectures used in recognition tasks. It also covers contemporary lightweight classification models used on edge devices. Lastly, we compare the performances of these architectures on multiple metrics.}
},
@article{awel2019review,
  title={Review on optical character recognition},
  author={Awel, Muna Ahmed and Abidi, Ali Imam},
  journal={International Research Journal of Engineering and Technology (IRJET)},
  volume={6},
  number={6},
  pages={3666--3669},
  year={2019}
},
@INPROCEEDINGS{6993174,
  author={Berchmans, Deepa and Kumar, S S},
  booktitle={2014 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)}, 
  title={Optical character recognition: An overview and an insight}, 
  year={2014},
  volume={},
  number={},
  pages={1361-1365},
  doi={10.1109/ICCICCT.2014.6993174}
  },
  @article{TONG2022104471,
title = {Deep learning-based detection from the perspective of small or tiny objects: A survey},
journal = {Image and Vision Computing},
volume = {123},
pages = {104471},
year = {2022},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2022.104471},
url = {https://www.sciencedirect.com/science/article/pii/S0262885622001007},
author = {Kang Tong and Yiquan Wu},
keywords = {Object detection, Small or tiny objects, Deep learning, Datasets, Convolutional neural networks},
abstract = {Detecting small or tiny objects is always a difficult and challenging issue in computer vision. In this paper, we provide a latest and comprehensive survey of deep learning-based detection approaches from the perspective of small or tiny objects. Our survey is featured by thorough and exhaustive analysis of small or tiny object detection. We comprehensively introduce 30 existing datasets about small or tiny objects, and summarize different definitions of small or tiny objects based on different application scenarios, such as pedestrian detection, traffic signs detection, face detection, remote sensing target detection and object detection in common life. Then small or tiny object detection techniques are overviewed systematically from seven aspects, including super-resolution techniques, context-based information, multi-scale representation learning, anchor mechanism, training strategy, data augmentation, and schemes based on loss function. Finally, the detection performance of small or tiny objects on 12 popular datasets is analyzed in depth. Based on performance analysis, we also discuss the promising research directions in the future. We hope this survey could provide researchers guidance to catalyze understanding of small or tiny object detection and further facilitate research on small or tiny object detection systems.}
},
@article{JI2021835,
title = {CNN-based encoder-decoder networks for salient object detection: A comprehensive review and recent advances},
journal = {Information Sciences},
volume = {546},
pages = {835-857},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520308926},
author = {Yuzhu Ji and Haijun Zhang and Zhao Zhang and Ming Liu},
keywords = {Salient object detection, Encoder-decoder model, Pixel-level classification, Video saliency, Empirical study},
abstract = {Convolutional neural network (CNN)-based encoder-decoder models have profoundly inspired recent works in the field of salient object detection (SOD). With the rapid development of encoder-decoder models with respect to most pixel-level dense prediction tasks, an empirical study still does not exist that evaluates performance by applying a large body of encoder-decoder models on SOD tasks. In this paper, instead of limiting our survey to SOD methods, a broader view is further presented from the perspective of fundamental architectures of key modules and structures in CNN-based encoder-decoder models for pixel-level dense prediction tasks. Moreover, we focus on performing SOD by leveraging deep encoder-decoder models, and present an extensive empirical study on baseline encoder-decoder models in terms of different encoder backbones, loss functions, training batch sizes, and attention structures. Moreover, state-of-the-art encoder-decoder models adopted from semantic segmentation and deep CNN-based SOD models are also investigated. New baseline models that can outperform state-of-the-art performance were discovered. In addition, these newly discovered baseline models were further evaluated on three video-based SOD benchmark datasets. Experimental results demonstrate the effectiveness of these baseline models on both image- and video-based SOD tasks. This empirical study is concluded by a comprehensive summary which provides suggestions on future perspectives.}
},
@article{mackenzie_examining_2009,
	title = {Examining the neural basis of episodic memory: {ERP} evidence that faces are recollected differently from names},
	volume = {47},
	issn = {00283932},
	shorttitle = {Examining the neural basis of episodic memory},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0028393209002383},
	doi = {10.1016/j.neuropsychologia.2009.05.025},
	language = {en},
	number = {13},
	urldate = {2023-01-18},
	journal = {Neuropsychologia},
	author = {MacKenzie, Graham and Donaldson, David I.},
	month = nov,
	year = {2009},
	pages = {2756--2765},
},
@article{tolba2006face,
  title={Face recognition: A literature review},
  author={Tolba, AS and El-Baz, AH and El-Harby, AA},
  journal={International journal of signal processing},
  volume={2},
  number={2},
  pages={88--103},
  year={2006},
  publisher={Citeseer}
},
@Article{electronics9081188,
AUTHOR = {Adjabi, Insaf and Ouahabi, Abdeldjalil and Benzaoui, Amir and Taleb-Ahmed, Abdelmalik},
TITLE = {Past, Present, and Future of Face Recognition: A Review},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1188},
URL = {https://www.mdpi.com/2079-9292/9/8/1188},
ISSN = {2079-9292},
ABSTRACT = {Face recognition is one of the most active research fields of computer vision and pattern recognition, with many practical and commercial applications including identification, access control, forensics, and human-computer interactions. However, identifying a face in a crowd raises serious questions about individual freedoms and poses ethical issues. Significant methods, algorithms, approaches, and databases have been proposed over recent years to study constrained and unconstrained face recognition. 2D approaches reached some degree of maturity and reported very high rates of recognition. This performance is achieved in controlled environments where the acquisition parameters are controlled, such as lighting, angle of view, and distance between the camera&ndash;subject. However, if the ambient conditions (e.g., lighting) or the facial appearance (e.g., pose or facial expression) change, this performance will degrade dramatically. 3D approaches were proposed as an alternative solution to the problems mentioned above. The advantage of 3D data lies in its invariance to pose and lighting conditions, which has enhanced recognition systems efficiency. 3D data, however, is somewhat sensitive to changes in facial expressions. This review presents the history of face recognition technology, the current state-of-the-art methodologies, and future directions. We specifically concentrate on the most recent databases, 2D and 3D face recognition methods. Besides, we pay particular attention to deep learning approach as it presents the actuality in this field. Open issues are examined and potential directions for research in facial recognition are proposed in order to provide the reader with a point of reference for topics that deserve consideration.},
DOI = {10.3390/electronics9081188}
},
@Article{s20020342,
AUTHOR = {Kortli, Yassin and Jridi, Maher and Al Falou, Ayman and Atri, Mohamed},
TITLE = {Face Recognition Systems: A Survey},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {342},
URL = {https://www.mdpi.com/1424-8220/20/2/342},
ISSN = {1424-8220},
ABSTRACT = {Over the past few decades, interest in theories and algorithms for face recognition has been growing rapidly. Video surveillance, criminal identification, building access control, and unmanned and autonomous vehicles are just a few examples of concrete applications that are gaining attraction among industries. Various techniques are being developed including local, holistic, and hybrid approaches, which provide a face image description using only a few face image features or the whole facial features. The main contribution of this survey is to review some well-known techniques for each approach and to give the taxonomy of their categories. In the paper, a detailed comparison between these techniques is exposed by listing the advantages and the disadvantages of their schemes in terms of robustness, accuracy, complexity, and discrimination. One interesting feature mentioned in the paper is about the database used for face recognition. An overview of the most commonly used databases, including those of supervised and unsupervised learning, is given. Numerical results of the most interesting techniques are given along with the context of experiments and challenges handled by these techniques. Finally, a solid discussion is given in the paper about future directions in terms of techniques to be used for face recognition.},
DOI = {10.3390/s20020342}
},
@inproceedings{serengil2020lightface,
  title        = {LightFace: A Hybrid Deep Face Recognition Framework},
  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},
  booktitle    = {2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},
  pages        = {23-27},
  year         = {2020},
  doi          = {10.1109/ASYU50717.2020.9259802},
  url          = {https://doi.org/10.1109/ASYU50717.2020.9259802},
  organization = {IEEE}
},
@article{guo2021sample,
  title={Sample and Computation Redistribution for Efficient Face Detection},
  author={Guo, Jia and Deng, Jiankang and Lattas, Alexandros and Zafeiriou, Stefanos},
  journal={arXiv preprint arXiv:2105.04714},
  year={2021}
}






  



  


